

A supernatural project creating an autonomous AI agent from scratch using Google's Gemini and python.

Basics of an AI agent:

1. A feedback loop or life cycle
2. Prompting an LLM with its own output to some extent

Those are the nuts and bolts of it.

A simple llm agent might look like this (psuedocode)

1. user asks LLM to do something
while True:
    2. LLM tasks itself to do something
    3. <do something using llm output>
    4. Ask LLM to review what happened, go back to 2

The interesting part of agents is that they can learn from mistakes (emphasis: the llm isn't learning, the system is) and do novel things.

Like executing code, back to our example from earlier:

1. input: "Send email to josh" + <some additional context about what to put in the email>
2. LLM is provided information about how to send an email (for example email_function(to_email: str, from_email: str, content: str))
3. LLM talks to itself in a loop (see example 1), adjusts email content until some condition is met, for example length or some qualitative factore 
3. LLM provides inputs to function: email_function(to_email: "josh@gmail.com", from_email: "gemini@gmail.com", content: str)
4. The function is executed using the llm's outputs
5. Agent bows and exits stage.

Anyone who got excited the first time they discovered the exec statement finds this fascinating. Agents are like a juiced up exec statements.

Looking at the email example, this just wasn't possible before LLMs and it is now. That's amazing.
It's also scary, what if the function wasn't an email function, but an actual exec statement? 
It's not hard to see a scenario where someone creates an agent with raw exec capabilties that is tasked with doing malicious things on the open internet, but that's not what this article is about.

2025 Prediction: Someone will do exactly this and it will be a newsworthy event <thinking emoji>

Today I will be talking about a unique way to build long lived agents using an actor model as well as some themes from OS kernels.

The key pieces of this approach are:
- A network of agents with one primary agent capable of spinning up/down secondary agents
- Tightly controlled, well established I/O resources for the agent network. No execs to the open internet.
- Memory using specific registers for subjects, managed by the primary agent.
- Controls for shared memory access for secondary agents
- A small interface for user interaction. User defines a goal for the primary agent as well as input/output devices and it handles the rest.

The end result is a shockingly smart AI agent with surprisingly light costs.
I recently got done watching all 15 seasons of the CW show Supernatural, so most of the code is supernatural themed. If this is annoying or you don't care for the show, please close the tab and read a different article.

The first thing people usually get wrong with calling out to LLMs via an API is async.
An API call to a provider like Google Gemini or Openai could take a few seconds.
In a regular program this is a show stopper, but when working with LLM's it's a fact of life.
At the end of the day we need that response back, but what do we if we want to get multiple responses back at the same time?
We can't just make a call, get a response, make a call, get a response. It would take too long, it's not time efficient.
This is where robust handling of async is required. Here I'm introducing something called the prompt - response map
You can think of it like instead of ordering 1 thing on Amazon and waiting for it to be delivered, then ordering the next thing,
you place orders for all the things you want right now and they are delivered at the same time.
Having a robust pattern for this is important for building LLM enabled apps.
So the pattern is this:
Give x number of prompts -> send out x number of requests -> get x number of responses

In python it looks like this:

async def pool_prompts(prompts: list[str]):
    tasks = [LLM.prompt(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)
    prompt_result_dict = dict(zip(prompts, results))
    return prompt_result_dict

You give it something like this:
prompts = [
    "Your first prompt here",
    "Your second prompt here",
    "Your third prompt here"
]

and it response with this:

{
    "Your first prompt here":"Response 1 ..."
    "Your second prompt here":"Response 2 ..."
    "Your third prompt here" "Response 3 ..."
}

... without blocking our precious python interpreter for each individual prompt.

So we will start with that. 





Drawbacks:
- Agents can be expensive
